{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自然言語処理デモ (ver. 2021)\n",
    "\n",
    "- 以下の資料は以下のYoutube動画を主に参考にしています：https://youtu.be/xvqsFTUsOmc \n",
    "    - GitHub: https://github.com/adashofdata/nlp-in-python-tutorial\n",
    "- 後半のBERTを用いた分析は以下の教科書を参考にしました：『BERTによる自然言語処理入門』\n",
    "    - https://www.amazon.co.jp/dp/B098J9M4PP/ref=dp_kinw_strp_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two standard text formats:\n",
    "\n",
    "1. **Corpus** - a collection of text (単語の現れる順番を保存したい: e.g., not happy)\n",
    "2. **Document-Term Matrix** - word counts in matrix format (単語の現れる頻度のみで十分: e.g., トピックモデリング)  \n",
    "$$\\begin{bmatrix} & word1 & word2 & word3 & \\dots \\\\\n",
    "text 1 & freq & freq & freq & \\dots\\\\\n",
    "text 2 & freq & freq & freq & \\dots\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting The Data\n",
    "\n",
    "- 自然言語処理は英語の方が日本語よりはるかに容易（単語の区切りがはっきりしているので）\n",
    "- 今回は日本のコロナ関連の英語ニュースを分析する：NHK World-Japan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web scraping, pickle imports\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "\n",
    "def url_to_text(url):\n",
    "    page = requests.get(url).text\n",
    "    soup = BeautifulSoup(page, \"html.parser\")\n",
    "    text = [p.text for p in soup.find(class_ = \"p-article__body\").find_all('p')]\n",
    "    # ソースの例：view-source:https://www3.nhk.or.jp/nhkworld/en/news/20211025_13/\n",
    "    print(url)\n",
    "    return text\n",
    "\n",
    "urls = ['https://www3.nhk.or.jp/nhkworld/en/news/20211025_13/',\n",
    "        'https://www3.nhk.or.jp/nhkworld/en/news/20211025_08/',\n",
    "        'https://www3.nhk.or.jp/nhkworld/en/news/20211025_04/',\n",
    "        'https://www3.nhk.or.jp/nhkworld/en/news/20211024_11/',\n",
    "        'https://www3.nhk.or.jp/nhkworld/en/news/20211024_13/',\n",
    "        'https://www3.nhk.or.jp/nhkworld/en/news/20211022_32/']\n",
    "\n",
    "# Articles: URLの最後の数字部分を取り出す\n",
    "\n",
    "articles = [url.split('/')[-2] for url in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 記事を取得\n",
    "\n",
    "# texts = [url_to_text(u) for u in urls] # 短時間にアクセスしすぎると繋がらなくなるので注意\n",
    "# texts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle files for later use\n",
    "\n",
    "# for i, c in enumerate(articles):\n",
    "#     with open(\"articles/\" + c + \".txt\", \"wb\") as file:\n",
    "#         pickle.dump(texts[i], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20211025_13',\n",
       " '20211025_08',\n",
       " '20211025_04',\n",
       " '20211024_11',\n",
       " '20211024_13',\n",
       " '20211022_32']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pickled files\n",
    "\n",
    "data = {}\n",
    "for i, c in enumerate(articles):\n",
    "    with open(\"articles/\" + c + \".txt\", \"rb\") as file:\n",
    "        data[c] = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['20211025_13', '20211025_08', '20211025_04', '20211024_11', '20211024_13', '20211022_32'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning The Data\n",
    "\n",
    "**Common data cleaning steps on all text:**\n",
    "* Make text all lower case\n",
    "* Remove punctuation\n",
    "* Remove numerical values\n",
    "* Remove common non-sensical text (/n)\n",
    "* Tokenize text\n",
    "* Remove stop words (たとえばin/on/atなどの前置詞)\n",
    "\n",
    "**More data cleaning steps after tokenization:**\n",
    "* Stemming / lemmatization (派生形・活用形の処理)\n",
    "* Create bi-grams or tri-grams (連続する複数単語でグループ化)\n",
    "* Deal with typos\n",
    "* And more..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Japan's wedding industry is enduring tough times as the coronavirus pandemic has been forcing couples to cancel their nuptials, and the organization that oversees the industry has been looking at who should bear the cost.\",\n",
       " 'The Bridal Institutional Association has revised its guidelines for wedding contracts for the first time in 13 years, introducing a clause on what to do in the event of an infectious disease outbreak.',\n",
       " 'The association is recommending that wedding companies allow rescheduling at no extra cost and reduce their fees for canceling a ceremony if the government has a business suspension request in place.',\n",
       " \"Japan's Consumer Affairs Center says it has received more than 5,400 complaints about wedding contracts since the start of the pandemic. Some of those cases have developed into lawsuits.\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataの値は１文ごとのリストになっている．これを全部まとめて文字列(str)形式にしたい．\n",
    "\n",
    "data['20211025_13']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_text(list_of_text):\n",
    "    '''Takes a list of text and combines them into one large chunk of text.'''\n",
    "    combined_text = ' '.join(list_of_text)\n",
    "    return combined_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Japan's wedding industry is enduring tough times as the coronavirus pandemic has been forcing couples to cancel their nuptials, and the organization that oversees the industry has been looking at who should bear the cost. The Bridal Institutional Association has revised its guidelines for wedding contracts for the first time in 13 years, introducing a clause on what to do in the event of an infectious disease outbreak. The association is recommending that wedding companies allow rescheduling at no extra cost and reduce their fees for canceling a ceremony if the government has a business suspension request in place. Japan's Consumer Affairs Center says it has received more than 5,400 complaints about wedding contracts since the start of the pandemic. Some of those cases have developed into lawsuits.\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine it!\n",
    "data_combined = {key: [combine_text(value)] for (key, value) in data.items()}\n",
    "data_combined['20211025_13'] # 文章が１まとめに繋がっている．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20211025_13</th>\n",
       "      <td>Japan's wedding industry is enduring tough tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20211025_08</th>\n",
       "      <td>The Tokyo Metropolitan Government has lifted a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20211025_04</th>\n",
       "      <td>Russia's first and second largest cities of Mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20211024_11</th>\n",
       "      <td>The government of Singapore will make COVID-19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20211024_13</th>\n",
       "      <td>Around 130 elementary school students took par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20211022_32</th>\n",
       "      <td>The Tokyo Metropolitan Government has opened t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       article\n",
       "20211025_13  Japan's wedding industry is enduring tough tim...\n",
       "20211025_08  The Tokyo Metropolitan Government has lifted a...\n",
       "20211025_04  Russia's first and second largest cities of Mo...\n",
       "20211024_11  The government of Singapore will make COVID-19...\n",
       "20211024_13  Around 130 elementary school students took par...\n",
       "20211022_32  The Tokyo Metropolitan Government has opened t..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataframe形式も準備しておく\n",
    "import pandas as pd\n",
    "\n",
    "data_df = pd.DataFrame.from_dict(data_combined).transpose()\n",
    "data_df.columns = ['article']\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データクリーニング\n",
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower() # 小文字にする\n",
    "    text = re.sub(r'[.|,|:|*|!|?|-|-|/]', '', text) # 記号\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text) # 数字を含む単語\n",
    "    text = re.sub(r'\\d', '', text) # 数字\n",
    "    return text\n",
    "\n",
    "clean = lambda x: clean_text(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20211025_13</th>\n",
       "      <td>japan's wedding industry is enduring tough tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20211025_08</th>\n",
       "      <td>the tokyo metropolitan government has lifted a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20211025_04</th>\n",
       "      <td>russia's first and second largest cities of mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20211024_11</th>\n",
       "      <td>the government of singapore will make  vaccina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20211024_13</th>\n",
       "      <td>around  elementary school students took part i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20211022_32</th>\n",
       "      <td>the tokyo metropolitan government has opened t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       article\n",
       "20211025_13  japan's wedding industry is enduring tough tim...\n",
       "20211025_08  the tokyo metropolitan government has lifted a...\n",
       "20211025_04  russia's first and second largest cities of mo...\n",
       "20211024_11  the government of singapore will make  vaccina...\n",
       "20211024_13  around  elementary school students took part i...\n",
       "20211022_32  the tokyo metropolitan government has opened t..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at the updated text\n",
    "data_clean = pd.DataFrame(data_df.article.apply(clean))\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus (コーパス)\n",
    "\n",
    "We already created a corpus in an earlier step. The definition of a corpus is a collection of texts, and they are all put together neatly in a pandas dataframe here.\n",
    "\n",
    "- 日本語の定義は　https://ja.wikipedia.org/wiki/%E3%82%B3%E3%83%BC%E3%83%91%E3%82%B9\n",
    "- つまり単に文章をたくさん集めたもの"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20211025_13</th>\n",
       "      <td>Japan's wedding industry is enduring tough tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20211025_08</th>\n",
       "      <td>The Tokyo Metropolitan Government has lifted a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20211025_04</th>\n",
       "      <td>Russia's first and second largest cities of Mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20211024_11</th>\n",
       "      <td>The government of Singapore will make COVID-19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20211024_13</th>\n",
       "      <td>Around 130 elementary school students took par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20211022_32</th>\n",
       "      <td>The Tokyo Metropolitan Government has opened t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       article\n",
       "20211025_13  Japan's wedding industry is enduring tough tim...\n",
       "20211025_08  The Tokyo Metropolitan Government has lifted a...\n",
       "20211025_04  Russia's first and second largest cities of Mo...\n",
       "20211024_11  The government of Singapore will make COVID-19...\n",
       "20211024_13  Around 130 elementary school students took par...\n",
       "20211022_32  The Tokyo Metropolitan Government has opened t..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at our dataframe\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pickle it for later use\n",
    "data_df.to_pickle(\"corpus.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document-Term Matrix（単語文書行列）\n",
    "\n",
    "For many of the techniques we'll be using in future notebooks, the text must be tokenized, meaning broken down into smaller pieces. The most common tokenization technique is to break down text into words. We can do this using scikit-learn's CountVectorizer, where every row will represent a different document and every column will represent a different word.\n",
    "\n",
    "In addition, with CountVectorizer, we can remove stop words. Stop words are common words that add no additional meaning to text such as 'a', 'the', etc.\n",
    "\n",
    "- Tokenization：トークン = 文章の構成要素．通常は１単語．場合によっては連続する複数単語(n-gram)．日本語はこのステップが難しい（形態素解析）．\n",
    "- 単語文書行列：それぞれのトークン（単語）の出現回数を行列にしたもの．文書数×単語数の行列なので，横に非常に長い行列になる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "# stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>accept</th>\n",
       "      <th>accordingly</th>\n",
       "      <th>activities</th>\n",
       "      <th>activity</th>\n",
       "      <th>affairs</th>\n",
       "      <th>aim</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>allow</th>\n",
       "      <th>allowed</th>\n",
       "      <th>...</th>\n",
       "      <th>ward</th>\n",
       "      <th>wedding</th>\n",
       "      <th>won</th>\n",
       "      <th>work</th>\n",
       "      <th>workplace</th>\n",
       "      <th>workplaces</th>\n",
       "      <th>world</th>\n",
       "      <th>year</th>\n",
       "      <th>yearend</th>\n",
       "      <th>years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20211025_13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20211025_08</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20211025_04</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20211024_11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20211024_13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20211022_32</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 346 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             able  accept  accordingly  activities  activity  affairs  aim  \\\n",
       "20211025_13     0       0            0           0         0        1    0   \n",
       "20211025_08     1       0            0           1         1        0    0   \n",
       "20211025_04     0       1            1           1         0        0    0   \n",
       "20211024_11     0       0            0           0         0        0    1   \n",
       "20211024_13     0       0            0           0         0        0    0   \n",
       "20211022_32     0       0            0           0         0        0    0   \n",
       "\n",
       "             alcohol  allow  allowed  ...    ward  wedding  won  work  \\\n",
       "20211025_13        0      1        0  ...       0        4    0     0   \n",
       "20211025_08        2      0        0  ...       0        0    0     0   \n",
       "20211025_04        0      0        1  ...       0        0    0     0   \n",
       "20211024_11        0      0        1  ...       0        0    0     1   \n",
       "20211024_13        0      0        0  ...       1        0    1     0   \n",
       "20211022_32        0      0        0  ...       2        0    0     1   \n",
       "\n",
       "             workplace  workplaces  world  year  yearend  years  \n",
       "20211025_13          0           0      0     0        0      1  \n",
       "20211025_08          0           0      0     0        1      0  \n",
       "20211025_04          0           0      0     1        0      0  \n",
       "20211024_11          1           1      0     1        0      0  \n",
       "20211024_13          0           0      1     1        0      1  \n",
       "20211022_32          0           0      0     0        0      1  \n",
       "\n",
       "[6 rows x 346 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We are going to create a document-term matrix using CountVectorizer, and exclude common English stop words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words = 'english')\n",
    "data_cv = cv.fit_transform(data_clean.article)\n",
    "data_dtm = pd.DataFrame(data_cv.toarray(), columns = cv.get_feature_names())\n",
    "data_dtm.index = data_clean.index\n",
    "data_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "#import nltk\n",
    "#nltk.download('punkt') # <= 初回は必要\n",
    "#nltk.download('wordnet') # <= 初回は必要\n",
    "#nltk.download('averaged_perceptron_tagger') # <= 初回は必要\n",
    "\n",
    "# Use TextBlob\n",
    "def textblob_tokenizer(str_input):\n",
    "    blob = TextBlob(str_input)\n",
    "    tag_dict = {\"J\": 'a', \n",
    "                \"N\": 'n', \n",
    "                \"V\": 'v', \n",
    "                \"R\": 'r'}\n",
    "    words_and_tags = [(w, tag_dict.get(pos[0], 'n')) for w, pos in blob.tags] # 単語の品詞を取得\n",
    "    words = [wd.lemmatize(tag) for wd, tag in words_and_tags] # lemmatize(): activities => activity\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>'s</th>\n",
       "      <th>able</th>\n",
       "      <th>accept</th>\n",
       "      <th>accordingly</th>\n",
       "      <th>activity</th>\n",
       "      <th>affair</th>\n",
       "      <th>aim</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>allow</th>\n",
       "      <th>announce</th>\n",
       "      <th>...</th>\n",
       "      <th>walkin</th>\n",
       "      <th>ward</th>\n",
       "      <th>wed</th>\n",
       "      <th>wedding</th>\n",
       "      <th>win</th>\n",
       "      <th>work</th>\n",
       "      <th>workplace</th>\n",
       "      <th>world</th>\n",
       "      <th>year</th>\n",
       "      <th>yearend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20211025_13</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20211025_08</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20211025_04</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20211024_11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20211024_13</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20211022_32</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 309 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             's  able  accept  accordingly  activity  affair  aim  alcohol  \\\n",
       "20211025_13   2     0       0            0         0       1    0        0   \n",
       "20211025_08   0     1       0            0         2       0    0        2   \n",
       "20211025_04   3     0       1            1         1       0    0        0   \n",
       "20211024_11   0     0       0            0         0       0    1        0   \n",
       "20211024_13   2     0       0            0         0       0    0        0   \n",
       "20211022_32   1     0       0            0         0       0    0        0   \n",
       "\n",
       "             allow  announce   ...     walkin  ward  wed  wedding  win  work  \\\n",
       "20211025_13      1         0   ...          0     0    3        1    0     0   \n",
       "20211025_08      0         0   ...          0     0    0        0    0     0   \n",
       "20211025_04      1         1   ...          0     0    0        0    0     0   \n",
       "20211024_11      1         1   ...          0     0    0        0    0     1   \n",
       "20211024_13      0         0   ...          0     1    0        0    1     0   \n",
       "20211022_32      0         0   ...          1     2    0        0    0     1   \n",
       "\n",
       "             workplace  world  year  yearend  \n",
       "20211025_13          0      0     1        0  \n",
       "20211025_08          0      0     0        1  \n",
       "20211025_04          0      0     1        0  \n",
       "20211024_11          2      0     1        0  \n",
       "20211024_13          0      1     2        0  \n",
       "20211022_32          0      0     1        0  \n",
       "\n",
       "[6 rows x 309 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(stop_words='english', tokenizer = textblob_tokenizer)\n",
    "data_cv = cv.fit_transform(data_clean.article)\n",
    "data_dtm = pd.DataFrame(data_cv.toarray(), columns = cv.get_feature_names())\n",
    "data_dtm.index = data_clean.index\n",
    "data_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pickle it for later use\n",
    "data_dtm.to_pickle(\"dtm.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also pickle the cleaned data (before we put it in document-term matrix format) and the CountVectorizer object\n",
    "data_clean.to_pickle('data_clean.pkl')\n",
    "pickle.dump(cv, open(\"cv.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
